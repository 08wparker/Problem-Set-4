---
title: "Problem Set 4"
author: "William Parker"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: '2'
  html_notebook:
    toc: yes
    toc_depth: 2
    toc_float: yes
---

For the following questions, use the world indicators data from class (`countries.csv`). _Be sure to prepare the data appropriately (e.g., standardize)._

```{r}
library(tidyverse)

library(psych) # for fa function
```


```{r}
countries <- read_csv("countries.csv")

names <- countries %>% select(name = X1)

countries %>% skimr::skim()
```

Looking over the variables, there are a bunch that clearly categorical variables. The factor analysis techniques we learned in class were using the Pearson's correlation matrix (assuming variables are continuous and follow multivariate normal distribution), so I'll select and standardize the continuous and normal (ish) variables only. Probably still violating assumptions. there seem to be extensions for ordinal variables (polychoric)

```{r}

countries <- countries %>%
  select(idealpoint, polity, democ, unreg, physint, new_empinx, gdp.pc.wdi, pop.wdi, milper, cinc) %>%
  mutate_all(function(x) as.numeric(scale(x))) 

countries
```

# Factor Analysis

## 1.	How do CFA and EFA differ?

In confirmatory factor analysis, the researcher is using the structure of the factor model to test a specific, well defined hypothesis. For example, using test results on math, physics, English, and Latin, a researcher could fit a 2-factor model to test the specific hypothesis that there are 2 types of latent "intelligence" variables, one correlated (at a given pre-specified) with math and physics and one correlated with English and Latin.

In exploratory factor analysis, the researcher does not have a defined hypothesis and instead is using factor analysis to explore strength of correlations in the data between features and discover how many latent dimensions are required to represent the data. For example, if the $k$ features are entirely independent, then EFA could reveal it take $k$ latent dimensions to properly represent the data


## 2.	Fit three exploratory factor analysis models initialized at 2, 3, and 4 factors. Present the loadings from these solutions and discuss in substantive terms. How does each fit? What sense does this give you of the underlying dimensionality of the space? And so on.

```{r}

fa_2 <- fa(countries, nfactors = 2)

fa_2$loadings
```

```{r}

fa_3 <- fa(countries, nfactors = 3)

fa_unrotated <- fa_3$loadings

fa_3$loadings
```

```{r}
fa_4 <- fa(countries, nfactors = 4)

fa_4$loadings
```

Inspecting the various number of factors, looks like the total proportion of variance explained jumps a lot from k = 2 to k =3, but not nearly as much when we add the 4 factor. This supports a 3 factor model.



## 3.	Rotate the 3-factor solution using any oblique method you would like and present a visual of the unrotated and rotated versions side-by-side. How do these differ and why does this matter (or not)?

I used the "varimax" rotation method
```{r rotated_3_factor}

fa_3_rotate <- fa(countries, 
                 nfactors = 3, 
                 rotate = "varimax") 

fa_rotated <- fa_3_rotate$loadings

fa_rotated
```


```{r}
compare_loadings <- as_tibble(fa_unrotated[,])%>%
  cbind(variable = names(countries)) %>%
  pivot_longer(cols = -variable, names_to = "factor", values_to = "loading_unrotated") %>%
  left_join(as_tibble(fa_rotated[,]) %>% 
              cbind(variable = names(countries)) %>%
              pivot_longer(cols = -variable, names_to = "factor", values_to = "loading_rotated"))


compare_loadings
```

```{r}
compare_loadings %>%
  ggplot(aes(x = loading_unrotated, y = loading_rotated, color = variable)) +
    geom_abline(intercept = 0, slope = 1, color="black", 
                 linetype="dashed", alpha = 0.5) +
  geom_point() + facet_wrap(~factor) 

```

Both of these factor analysis models explain the same proportion of variance in the data, so in that sense they are equivalent. 

Also the magnitude of the loadings on each factor are roughly the same for most variables, so it doesn't appear this rotation has changed the interpretation of each factor significantly.



# Principal Components Analysis

## 1.	What is the statistical difference between PCA and FA? Describe the basic construction of each approach using equations and then point to differences that exist across these two widely used methods for reducing dimensionality.

### Factor Analysis
Let's define $x_1, x_2,..x_n$ to be our sample where $x_i \in R^d$, i.e. we have $d$ features. Let the matrix $X$ represent the whole sample with dimensions d x n (after scaling to mean zero for each feature).

In factor analysis, we represent 

$X  = LF + \varepsilon$

where $L$ is a d x k of "loadings" and $F$ is a k x n matrix representing the unobserved latent "factors" and $\varepsilon$ is a d x k matrix of error terms. The loadings $L$ here do not vary with $n$ and represent the relationship between the unobserved factors $F$ and the $d$ input feature variables. 

focusing on a single observation from the sample $x_i$ is a d x 1 vector

$x_i = LF_i + \varepsilon_i$

where $F_i$ is the ith column of $F$ and is a k x 1 vector of weights.


### PCA 
In PCA, we minimize the following loss function

$Loss = \sum_{i=1}^n||x_i - V\alpha_i||^2$

where $V$ is an **orthogonal** d x k matrix (turns out to be the top k eigenvectors of the sample covariance $S = \frac{1}{n}\sum x_ix_i^t$) and the columns are called the principle component "vectors". $\alpha_i$ is a k x 1 vector representing the weight of each component in creating the estimate of $x_i$.

so $V$ is analogous to the loading matrix $L$ and $\alpha_i$ is analogous to the weighting vector $F_i$ for observation $x_i$.

### Differences
In factor analysis, there are many possible ways to optimize $L$ and $F$ with respect to various loss functions and restrictions on $L$. The orthogonality assumption of $V$ and the specification of a particular loss function is what distinguishes PCA from (and makes it a a subtype of) factor analysis. 


## 2.	Fit a PCA model. Present the proportion of explained variance across the first 10 components. What do these values tell you substantively (e.g., how many components likely characterize these data?)?

```{r}
cov_matrix <- cov(countries)
country_eigen <- eigen(cov_matrix)
cov_matrix
```

```{r}
prcomp(countries)
```


```{r}
summary(prcomp(countries))
```
I used the `prcomp` function to get the proportion of explained variance quickly. Looks like 3 components characterize the data fairly well (80% proportion of variance), similar to what I found in the factor analysis.


## 3.	Present a biplot of the PCA fit from the previous question. Describe what you see (e.g., which countries are clustered together? Which input features are doing the bulk of the explaining? How do you know this?

```{r}
pca_project <- as.matrix(countries) %*% as.matrix(country_eigen$vectors) %>% 
  as_tibble() %>%
  cbind(country = names$name) %>%
  cbind(countries)

head(pca_project)

pca_project %>%
  mutate(country = if_else(V2 < -2 | V1 > 4, as.character(country), ""))%>% 
  ggplot(aes(x = V1, y = V2, label = country, color = democ)) +
  geom_point() + ggrepel::geom_label_repel(label.size = 0.05) +
  labs( x= "PC 1", y = "PC 2")
```
`democ` is correlated with PC1 (along with a few other features strongly correlated with democracy) and country size seems to be strongly correlated with PC 2. 

so type of government is PC1, country size is PC2.
